# Agent Testing Plan

This document outlines how we'll test each of our subagents with the mock project.

## Important Note About Agent Invocation

Based on our v1.0.0 testing, we learned that to properly test the actual agents (rather than simulating them with general-purpose agents), you must:

1. Navigate to this project directory: `cd ./agent-testing-project` (or whatever you've named this directory)
2. Run Qwen Code: `qwen`
3. Invoke agents directly using the `@` syntax from within the Qwen Code TUI

This ensures the agents are properly recognized and can use their defined tools.

## 1. Code Reviewer Agent

**Test Scenario**: Review the authentication service for general code quality

**Invocation Command**: 
```
Review the authentication service for general code quality
```

**Expected Actions**:
- Analyze code structure and organization
- Identify performance issues
- Check for best practices
- Evaluate error handling
- Assess readability and maintainability
- Provide feedback on testability

**Files to Review**:
- `src/authService.js`
- `src/index.js`

## 2. Security Code Reviewer Agent

**Test Scenario**: Review the authentication service for security vulnerabilities

**Invocation Command**: 
```
Review the authentication service for security vulnerabilities
```

**Expected Actions**:
- Identify security vulnerabilities in the code
- Check for proper input validation
- Evaluate authentication and authorization implementation
- Assess data handling practices
- Look for information disclosure risks
- Provide remediation recommendations

**Files to Review**:
- `src/authService.js`
- `src/index.js`

## 3. Testing Expert Agent

**Test Scenario**: Create unit tests for the authentication service

**Invocation Command**: 
```
Create comprehensive unit tests for the authentication service
```

**Expected Actions**:
- Analyze the code structure and dependencies
- Identify key functionality and edge cases
- Create comprehensive test suites
- Include setup/teardown and meaningful assertions
- Add comments explaining complex test scenarios
- Ensure tests are maintainable and follow DRY principles

**Files to Work With**:
- `src/authService.js`
- `tests/authService.test.js` (extend this file)

## 4. Documentation Writer Agent

**Test Scenario**: Create API documentation for the authentication service

**Invocation Command**: 
```
Create API documentation for the authentication service
```

**Expected Actions**:
- Analyze the code to understand endpoints and functionality
- Create clear endpoint descriptions with examples
- Document parameter details with types and constraints
- Explain response formats
- Document error code explanations
- Include authentication requirements

**Files to Work With**:
- `src/index.js` (API endpoints)
- `docs/api.md` (extend this file)

## Testing Procedure

1. **Setup**: Navigate to project directory and run `qwen`
2. **Invoke**: Run each agent by simply typing the prompt command directly (without the `@agent-name.md` syntax)
3. **Observe**: Monitor the agent's ability to use its defined tools
4. **Verify**: Check the quality and completeness of the output
5. **Validate**: Confirm that the agent stays within its defined scope
6. **Document**: Record any issues or limitations found

## Testing Workflow

We use a specific git workflow to ensure consistent testing conditions:

### Branch Structure
- **`main`**: Contains stable project files, documentation, and accumulated test results
- **`clean`**: Always reset to initial commit (c56b545c5d32230a3f553850390121dfc893513d) for pristine testing

### Automated Workflow Script
We have an automation script that simplifies the testing workflow:

```bash
# Start a clean testing environment
./scripts/agent-test.sh start

# After testing, prepare for results documentation
./scripts/agent-test.sh finish
```

The script automatically:
1. Detects the next test version number (v1, v2, v3, etc.)
2. Switches to the main branch
3. Creates a new test-results directory for this version
4. Generates a README.md file from our flexible template
5. Preserves test artifacts in the artifacts subdirectory
6. Prepares for committing results

### Manual Workflow (if script is not available)
1. **Before Each Test Session**:
   ```bash
   git checkout clean
   git reset --hard c56b545c5d32230a3f553850390121dfc893513d
   ```

2. **During Testing**: Run your agents in Qwen Code

3. **After Testing**:
   ```bash
   git checkout main
   # Create version directory and commit results
   mkdir -p test-results/vX
   # Create README.md from template (see test-results/template.md)
   git add test-results/vX/
   git commit -m "Document vX test results"
   ```

### Artifact Preservation
Files generated by agents during testing are preserved in versioned directories:
- `test-results/v1/artifacts/` - Artifacts from v1 test
- `test-results/v2/artifacts/` - Artifacts from v2 test
- `test-results/v3/artifacts/` - Artifacts from v3 test (future)

Additionally, each test version directory contains a README.md file that documents the test results. This file is generated from a flexible template that can accommodate varying numbers and types of agents, ensuring consistency while allowing for future expansion.

This maintains the original directory structure while keeping artifacts and documentation organized.

### Benefits of This Approach
1. **Consistency**: Each test starts from exactly the same state
2. **Isolation**: Test artifacts don't interfere with future tests
3. **Documentation**: Results are preserved for future reference
4. **Scalability**: Can handle multiple test iterations
5. **Automation**: Simple commands handle complex workflow
6. **Sandbox Compatibility**: Works within Qwen Code's project directory constraints

## Version Numbering Convention

We use simple sequential versioning for our test iterations:
- **v1**: First test (our v1.0.0 simulation)
- **v2**: Second test (first actual agent test)
- **v3**: Third test (second actual agent test)
- And so on...

The automation script automatically detects the next version number.

## Expected Differences from v1.0.0 Simulation

When testing with actual agents rather than general-purpose simulations:

1. **Tool Usage**: Agents will use only their defined tools (e.g., `read_file`, `write_file`) rather than having access to all general-purpose tools
2. **Scope Adherence**: Agents will be strictly constrained to their specialized focus areas
3. **Output Format**: Agents will produce output in their specific formats as defined in their configurations
4. **Performance**: Agents may have different performance characteristics with their constrained toolsets

## Troubleshooting

If agents are not recognized:
1. Ensure you're running `qwen` from within the project directory
2. Verify that agent files exist in the appropriate agents directory (typically `~/.qwen/agents/` or as configured in your environment)
3. Check that agent files have the correct YAML frontmatter format
4. Restart Qwen Code if agents still aren't recognized

## Version History

- **v1.0.0** (2025-09-14): Initial testing plan
- **v1.1.0** (2025-09-14): Updated with proper agent invocation instructions and testing workflow based on simulation results
- **v1.2.0** (2025-09-15): Updated agent invocation commands to reflect actual usage pattern and corrected manual workflow instructions