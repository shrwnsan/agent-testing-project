# Agent Testing Plan

This document outlines how we'll test each of our subagents with the mock project.

## Important Note About Agent Invocation

Based on our v1.0.0 testing, we learned that to properly test the actual agents (rather than simulating them with general-purpose agents), you must:

1. Navigate to this project directory: `cd ./test-project-for-agents` (or whatever you've named this directory)
2. Run Qwen Code: `qwen`
3. Invoke agents directly using the `@` syntax from within the Qwen Code TUI

This ensures the agents are properly recognized and can use their defined tools.

## 1. Code Reviewer Agent

**Test Scenario**: Review the authentication service for general code quality

**Invocation Command**: 
```
@code-reviewer.md Review the authentication service for general code quality
```

**Expected Actions**:
- Analyze code structure and organization
- Identify performance issues
- Check for best practices
- Evaluate error handling
- Assess readability and maintainability
- Provide feedback on testability

**Files to Review**:
- `src/authService.js`
- `src/index.js`

## 2. Security Code Reviewer Agent

**Test Scenario**: Review the authentication service for security vulnerabilities

**Invocation Command**: 
```
@security-code-reviewer.md Review the authentication service for security vulnerabilities
```

**Expected Actions**:
- Identify security vulnerabilities in the code
- Check for proper input validation
- Evaluate authentication and authorization implementation
- Assess data handling practices
- Look for information disclosure risks
- Provide remediation recommendations

**Files to Review**:
- `src/authService.js`
- `src/index.js`

## 3. Testing Expert Agent

**Test Scenario**: Create unit tests for the authentication service

**Invocation Command**: 
```
@testing-expert.md Create comprehensive unit tests for the authentication service
```

**Expected Actions**:
- Analyze the code structure and dependencies
- Identify key functionality and edge cases
- Create comprehensive test suites
- Include setup/teardown and meaningful assertions
- Add comments explaining complex test scenarios
- Ensure tests are maintainable and follow DRY principles

**Files to Work With**:
- `src/authService.js`
- `tests/authService.test.js` (extend this file)

## 4. Documentation Writer Agent

**Test Scenario**: Create API documentation for the authentication service

**Invocation Command**: 
```
@documentation-writer.md Create API documentation for the authentication service
```

**Expected Actions**:
- Analyze the code to understand endpoints and functionality
- Create clear endpoint descriptions with examples
- Document parameter details with types and constraints
- Explain response formats
- Document error code explanations
- Include authentication requirements

**Files to Work With**:
- `src/index.js` (API endpoints)
- `docs/api.md` (extend this file)

## Testing Procedure

1. **Setup**: Navigate to project directory and run `qwen`
2. **Invoke**: Run each agent using the specific `@agent-name.md` syntax
3. **Observe**: Monitor the agent's ability to use its defined tools
4. **Verify**: Check the quality and completeness of the output
5. **Validate**: Confirm that the agent stays within its defined scope
6. **Document**: Record any issues or limitations found

## Testing Workflow

We use a specific git workflow to ensure consistent testing conditions:

### Branch Structure
- **`main`**: Contains stable project files, documentation, and accumulated test results
- **`clean`**: Always reset to initial commit (387f041) for pristine testing

### Before Each Test Session
```bash
git checkout clean
git reset --hard 387f041
```

This ensures a consistent, clean state for testing.

### During Testing
Run your agents in Qwen Code, which will modify files in the project directory.

### After Testing
1. Preserve valuable results:
   ```bash
   git checkout main
   # Copy valuable files or commit documentation
   ```

2. Commit valuable documentation:
   ```bash
   git add test-results/vX.Y.Z/
   git commit -m "Document vX.Y.Z test results"
   ```

### Artifact Preservation
Files generated by agents during testing are preserved in versioned directories:
- `test-results/v1.0.0/artifacts/` - Artifacts from v1.0.0 test
- `test-results/v2.0.0/artifacts/` - Artifacts from v2.0.0 test (future)

This maintains the original directory structure while keeping artifacts organized.

### Benefits of This Approach
1. **Consistency**: Each test starts from exactly the same state
2. **Isolation**: Test artifacts don't interfere with future tests
3. **Documentation**: Results are preserved for future reference
4. **Scalability**: Can handle multiple test iterations
5. **Sandbox Compatibility**: Works within Qwen Code's project directory constraints

## Expected Differences from v1.0.0 Simulation

When testing with actual agents rather than general-purpose simulations:

1. **Tool Usage**: Agents will use only their defined tools (e.g., `read_file`, `write_file`) rather than having access to all general-purpose tools
2. **Scope Adherence**: Agents will be strictly constrained to their specialized focus areas
3. **Output Format**: Agents will produce output in their specific formats as defined in their configurations
4. **Performance**: Agents may have different performance characteristics with their constrained toolsets

## Troubleshooting

If agents are not recognized:
1. Ensure you're running `qwen` from within the project directory
2. Verify that agent files exist in the appropriate agents directory (typically `~/.qwen/agents/` or as configured in your environment)
3. Check that agent files have the correct YAML frontmatter format
4. Restart Qwen Code if agents still aren't recognized

## Version History

- **v1.0.0** (2025-09-14): Initial testing plan
- **v1.1.0** (2025-09-14): Updated with proper agent invocation instructions and testing workflow based on simulation results