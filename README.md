# Agent Testing Project

This is a comprehensive testing framework designed to evaluate and demonstrate the capabilities of specialized AI coding agents. What started as a simple "vibe coding" experiment to test if agent invocation was working properly has evolved into a robust benchmarking tool for AI-assisted development.

Originally created for Qwen Code's subagent team, this project provides a standardized environment for testing agent performance across multiple dimensions:

1. Code Reviewer
2. Security Code Reviewer
3. Testing Expert
4. Documentation Writer

## The Origin Story

This project began as a humble exploration - a "vibe coding" session where we simply wanted to see if we could get specialized agents to work together effectively. What started as an experimental playground to test the basic functionality of agent invocation has grown into a comprehensive framework for evaluating AI coding assistants.

The beauty of this project lies in its simplicity. We created an intentionally flawed authentication service with deliberate code quality and security issues, providing a realistic environment for testing agent capabilities. Through iterative experimentation and refinement, we discovered that this simple approach could effectively reveal the strengths and weaknesses of different AI agents.

## Project Structure

- `src/` - Source code directory containing the mock authentication service
- `tests/` - Test files for the authentication service
- `docs/` - Documentation files, including API documentation generated by agents
- `test-results/` - Versioned test results and documentation organized by test iteration
- `scripts/` - Automation scripts for consistent testing workflows
- `package.json` - Project configuration and dependencies
- `CHANGELOG.md` - Project change history and version tracking

## Test Scenarios

1. **Code Reviewer**: Review the authentication service for general code quality
2. **Security Code Reviewer**: Review the authentication service for security vulnerabilities
3. **Testing Expert**: Create unit tests for the authentication service
4. **Documentation Writer**: Create API documentation for the authentication service

This project is intentionally simple but contains enough complexity to thoroughly test all agent capabilities, including their ability to identify security vulnerabilities, code quality issues, and generate comprehensive documentation and tests.

## Test Results

Test results are stored in the `test-results/` directory, organized by version:

- `test-results/v1/` - First comprehensive test using general-purpose agents for simulation (September 2025)
- `test-results/v2/` - Second comprehensive test using actual specialized agents (September 2025)

**Important**: The v1 test used general-purpose agents to simulate our specialized agents rather than invoking the actual agent files directly. The v2 test used actual specialized agents. For detailed information about the test results and methodology, see `test-results/v1/README.md` and `test-results/v2/README.md`.

Each test version directory contains a README.md file that summarizes the results and methodology of that particular test trial, along with preserved artifacts in the `artifacts/` subdirectory.

## Using This Framework with Other AI Coding Assistants

This testing framework can be adapted for other AI coding assistants with minimal modifications (Qwen Code, Claude Code, OpenAI Codex, Gemini CLI, OpenCode, etc.). The key is to provide each AI assistant with clear role definitions, specific instructions, and access to appropriate tools for each specialized task.


## Quick Start Guide

Want to test your own AI agents with this framework? Here's how to get started:

### 1. Clone the Repository

```bash
git clone https://github.com/shrwnsan/agent-testing-project.git
cd agent-testing-project
```

### 2. Install Dependencies

```bash
npm install
```

### 3. Understand the Test Subject

The framework includes a mock authentication service with intentional flaws:
- Explore `src/authService.js` and `src/index.js` to understand the codebase
- Review `tests/authService.test.js` to see the existing test suite
- Check `docs/api.md` for API documentation (generated by agents)

### 4. Set Up Testing Environment

Use our automation script to prepare a clean testing environment:

```bash
./scripts/agent-test.sh start
```

This command:
- Checks out the clean branch
- Resets to a predefined clean state
- Determines the next version number for your test
- Prepares a directory for your test results

### 5. Run Your Agents

With your AI coding assistant environment set up:
1. Run your specialized agents against the authentication service
2. Assign them tasks similar to our test scenarios:
   - Code Reviewer: Analyze code quality and structure
   - Security Reviewer: Identify vulnerabilities and security issues
   - Testing Expert: Create or enhance unit tests
   - Documentation Writer: Generate or improve API documentation

### 6. Preserve Your Results

After your agents have completed their work:

```bash
./scripts/agent-test.sh finish
```

This command:
- Detects all files modified by your agents
- Preserves artifacts in a versioned directory
- Switches back to the main branch
- Generates a template README.md for your results

### 7. Document and Share

1. Review the preserved artifacts in `test-results/vX/artifacts/`
2. Update `test-results/vX/README.md` with your findings (get help from your AI agent)
3. Commit your results (optional)
   ```bash
   git add test-results/vX/
   git commit -m "Document vX test results"
   ```

## Automated Testing

```bash
npm install
```

### 3. Understand the Test Subject

The framework includes a mock authentication service with intentional flaws:
- Explore `src/authService.js` and `src/index.js` to understand the codebase
- Review `tests/authService.test.js` to see the existing test suite
- Check `docs/api.md` for API documentation (generated by agents)

### 4. Set Up Testing Environment

Use our automation script to prepare a clean testing environment:

```bash
./scripts/agent-test.sh start
```

This command:
- Checks out the clean branch
- Resets to a predefined clean state
- Determines the next version number for your test
- Prepares a directory for your test results

### 5. Run Your Agents

With your AI coding assistant environment set up:
1. Run your specialized agents against the authentication service
2. Assign them tasks similar to our test scenarios:
   - Code Reviewer: Analyze code quality and structure
   - Security Reviewer: Identify vulnerabilities and security issues
   - Testing Expert: Create or enhance unit tests
   - Documentation Writer: Generate or improve API documentation

### 6. Preserve Your Results

After your agents have completed their work:

```bash
./scripts/agent-test.sh finish
```

This command:
- Detects all files modified by your agents
- Preserves artifacts in a versioned directory
- Switches back to the main branch
- Generates a template README.md for your results

### 7. Document and Share

1. Review the preserved artifacts in `test-results/vX/artifacts/`
2. Update `test-results/vX/README.md` with your findings (get help from your AI agent)
3. Commit your results (optional)
   ```bash
   git add test-results/vX/
   git commit -m "Document vX test results"
   ```

## Automated Testing

We provide an automation script to simplify the testing workflow:

```bash
# Start a clean testing environment
./scripts/agent-test.sh start

# After testing, prepare for results documentation
./scripts/agent-test.sh finish
```

The script automatically handles version numbering and artifact preservation, ensuring consistent testing conditions for each evaluation.

### Testing Workflow

The following diagram illustrates the complete testing workflow when using our automation script:

```mermaid
flowchart TD
    A[Start] --> B[Run ./scripts/agent-test.sh start]
    B --> C[Script checks out clean branch]
    C --> D[Resets to predefined clean state]
    D --> E[Determines next version number]
    E --> F[Creates version directory]
    F --> G[Ready for agent testing]
    G --> H[User runs specialized agents]
    H --> I[Agents modify files in repository]
    I --> J[Run ./scripts/agent-test.sh finish]
    J --> K[Script detects modified files]
    K --> L[Preserves artifacts in version directory]
    L --> M[Switches to main branch]
    M --> N[Generates results README from template]
    N --> O[Test results ready for review]
    O --> P[User updates README with actual results]
    P --> Q[Commit and document test results]
    Q --> R[End]
```

This workflow ensures:
1. **Consistency**: Each test starts from exactly the same clean state
2. **Isolation**: Test artifacts are preserved separately for each version
3. **Documentation**: Results are automatically templated for consistent reporting
4. **Reproducibility**: The exact same setup can be recreated for verification

## Contributing

This project is intentionally kept simple and lightweight. Feel free to fork the repository and modify it for your own testing purposes. If you have improvements that could benefit the wider community, please feel free to submit a pull request.
